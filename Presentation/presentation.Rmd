---
title: "Final Project"
author: "Kushal Ismael and Matt Kosko"
date: "6/23/2021"
bibliography: ml.bib
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(data.table)
library(kableExtra)
```

# Predicting Violent Crime Rates

## Dataset
- Data hosted on the University of California Irvine machine learning repository [@redmond2009crime]
- Combines data from three datasets, the 1990 Census, 1995 FBI Uniform Crime Report (UCR), and the 1990 US Law Enforcement Management and Administrative Statistics Survey (LEMAS) 
- Contain information about socio-economic indicators (e.g., median family income) and crime/law enforcement (e.g., per capita number of police officers)
- 1994 observations and 122 predictive features. 

## Problem Statement
- In this project, we want to predict violent crime rates from a variety of features using the "Communities and Crime" dataset
- Main challenges are processing the dataset, choosing predictors, and models

## Target

```{r histogram, out.width="65%", fig.align='center'}
knitr::include_graphics('/home/mkosko/PycharmProjects/machineLearning/histogram.png')
```

<!-- ## Target -->

<!-- ```{r violent, out.width="65%", fig.align='center'} -->
<!-- knitr::include_graphics('/home/mkosko/PycharmProjects/machineLearning/violent.png') -->
<!-- ``` -->


## Plan
- Choose a method for dealing with missing data
- Filter features to choose the most relevant
- Pick the best model
  - Compare MLP with classical machine learning methods


## Contribution
- This data set does not have any published papers relevant to it [@redmond2009crime]
  - So any analysis with this particular data set is new
- There is a large literature on the use of machine learning in so-called "predictive policing" [@hardyns2018predictive]
- Many predictive policing systems use a single method ML method, like gradient boosting
- We have tried many different methods and compared them

## Missing Data
- Data are "missing not at random" [@rubin1976inference]
- Impute data using *MICE* algorithm [@van2018flexible]
  - Implemented as part of `MultipeImputer`
- Check analysis as part of robustness check

## Preprocessing
- Filter features based on correlation, drop variables with correlation greater than a cutoff value 
- Algorithm for removing highly correlated features is taken from [@kuhn2013applied, p. 47]

1. Calculate correlation matrix (`.corr()`)
2. Find two predictors with largest absolute pairwise correlation (A and B)
3. Find average correlation for all features with A and all features with B
4. If A greater than B, remove A. Else, B
5. Repeat until no correlations above threshold


## Solver
- Rather than stochastic gradient descent, use `L-BFGS` solver (limited memory Broyden–Fletcher–Goldfarb–Shanno) [@aggarwal2018neural, p. 148]
- Approximates Newton method: 

\[
\mathbf{W}(t+1) = \mathbf{W}(t) - \mathbf{H}^{-1}\nabla F(t)
\]

- Replace $\mathbf{H}^{-1}$ with an approximation $\mathbf{G}(t)$
\[
\mathbf{W}(t+1) = \mathbf{W}(t) - \alpha(t)\mathbf{G}(t)\nabla F(t)
\]

## Hyperparameter Tuning
- For every model type, there are many parameters to choose
  - For MLP, can choose number of hidden layers, activation function, learning rate, etc.
- We use 5-fold cross validation to choose hyperparameters for each model type
- Implemented with `GridSearchCV`, scored by `neg_mean_squared_error`


# Results

## Model Architecture

```{r our-mlp, out.width="99%", fig.align='left', fig.cap = "Chosen MLP"}
knitr::include_graphics('MLP_Network_Drawing.png')
```

## Results

```{r non}
non <- fread('/home/mkosko/PycharmProjects/machineLearning/modeleval-full.csv')
impute <- fread('/home/mkosko/PycharmProjects/machineLearning/modeleval-impute.csv')
non[, `Negative Mean Squared Error` := abs(`Negative Mean Squared Error`)]
impute[, `Negative Mean Squared Error` := abs(`Negative Mean Squared Error`)]

non[, 1:3] %>%
  kbl(caption = "Model Performance for Non-Imputed Data", digits = 4, 
      col.names = c('Model Type', 'Best 5-Fold MSE', 'Test MSE'), 
      escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r impute}
impute[, 1:3] %>%
  kbl(caption = "Model Performance for Imputed Data", digits = 4, 
      col.names = c('Model Type', 'Best 5-Fold MSE', 'Test MSE'), 
      escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

## Predicted Values
```{r predict, out.width="60%", fig.align='center', fig.cap = "Violent Crime Per Population Comparison (Test Data)"}
knitr::include_graphics('/home/mkosko/PycharmProjects/machineLearning/Histogram_Comparison.png')
```

## Predicted Crime Rate Changes

```{r features, echo=FALSE,out.width="49%", out.height="40%", fig.show='hold',fig.align='center'}
knitr::include_graphics(c('/home/mkosko/PycharmProjects/machineLearning/Scatter-PopDens.png',
                          '/home/mkosko/PycharmProjects/machineLearning/Scatter-LandArea.png',
                          '/home/mkosko/PycharmProjects/machineLearning/Scatter-PctUnemployed.png',
                          '/home/mkosko/PycharmProjects/machineLearning/Scatter-PctEmploy.png'))
``` 


## Further Work
- Better data imputation algorithm
  - `IterativeImputer` has multiple estimations methods or `KNNImputer`
- More up to date data
  - Potentially incorporate time series data
- More comprehensive hyperparameter grid to search

## References {.allowframebreaks}